# M06 Context Integrator êµ¬í˜„ ê°€ì´ë“œ

## ë¬¸ì„œ ë¶„ë¥˜

ì´ ë¬¸ì„œëŠ” **êµ¬í˜„ ê°€ì´ë“œ**ì…ë‹ˆë‹¤.
- ğŸ“š **ì •ë³´ ìë£Œ**: `M06_RESEARCH_MATERIALS.md`
- ğŸ“– **êµ¬í˜„ ê°€ì´ë“œ**: ë³¸ ë¬¸ì„œ (M06_IMPLEMENTATION_GUIDE.md)
- ğŸ’» **í”„ë¡œì íŠ¸ ì½”ë“œ**: `seeds/molecular/m06_context_integrator.py`
- ğŸ§ª **í™œìš© ì˜ˆì œ**: `examples/m06_usage_examples.py`

---

## ëª©ì°¨

1. [ê°œìš”](#1-ê°œìš”)
2. [ì„¤ê³„ ëª…ì„¸](#2-ì„¤ê³„-ëª…ì„¸)
3. [ë‹¨ê³„ë³„ êµ¬í˜„ ê°€ì´ë“œ](#3-ë‹¨ê³„ë³„-êµ¬í˜„-ê°€ì´ë“œ)
4. [í…ŒìŠ¤íŠ¸ ì „ëµ](#4-í…ŒìŠ¤íŠ¸-ì „ëµ)
5. [ì„±ëŠ¥ ìµœì í™”](#5-ì„±ëŠ¥-ìµœì í™”)
6. [ì°¸ê³  ìë£Œ](#6-ì°¸ê³ -ìë£Œ)

---

## 1. ê°œìš”

### 1.1 ê¸°ë³¸ ì •ë³´

- **ì‹œë“œ ID**: SEED-M06
- **ì´ë¦„**: Context Integrator
- **Level**: 1 (Molecular)
- **Category**: Composition
- **Target Params**: ~650K
- **Bit Depth**: FP8

### 1.2 ëª©ì 

êµ­ì†Œì  ë§¥ë½(local context)ê³¼ ì „ì—­ì  ë§¥ë½(global context)ì„ ìœµí•©í•˜ì—¬ **ì¤‘ì˜ì„±ì„ í•´ì†Œ**í•˜ê³  ì´í•´ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

### 1.3 êµ¬ì„± ì‹œë“œ

- **A06**: Sequence Tracker (ì‹œê°„ì  ë§¥ë½)
- **M01**: Hierarchy Builder (ê³„ì¸µì  ë§¥ë½)
- **A05**: Grouping Nucleus (ê·¸ë£¹ ë§¥ë½)

### 1.4 í•µì‹¬ ê¸°ëŠ¥

1. **Multi-scale Context Encoding**
   - Local context: ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ê¸°ë°˜
   - Global context: ì „ì²´ ì‹œí€€ìŠ¤ ê¸°ë°˜

2. **Hierarchical Context Integration**
   - Temporal context (A06)
   - Hierarchical context (M01)
   - Group context (A05)

3. **Context Fusion**
   - Multi-head attention ê¸°ë°˜
   - Cross-attention mechanism

4. **Disambiguation**
   - ë§¥ë½ ê¸°ë°˜ ì¤‘ì˜ì„± í•´ì†Œ
   - ìƒí˜¸ì‘ìš© íŠ¹ì§• í™œìš©

---

## 2. ì„¤ê³„ ëª…ì„¸

### 2.1 ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

```
Input [B, L, D]
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                 â”‚
    â–¼                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Atomic Seeds   â”‚         â”‚  Local/Global    â”‚
â”‚  (ë³‘ë ¬ ì²˜ë¦¬)     â”‚         â”‚  Context Encoder â”‚
â”‚  - A06 (ì‹œê°„)   â”‚         â”‚  - Transformer   â”‚
â”‚  - M01 (ê³„ì¸µ)   â”‚         â”‚  - Sliding Windowâ”‚
â”‚  - A05 (ê·¸ë£¹)   â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
    â”‚                               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Context Fusionâ”‚
        â”‚ (Multi-head   â”‚
        â”‚  Attention)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Disambiguator â”‚
        â”‚ (ì¤‘ì˜ì„± í•´ì†Œ) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
        Output [B, L, D]
```

### 2.2 ì…ì¶œë ¥ ëª…ì„¸

#### ì…ë ¥
- `x`: `[B, L, D]` - ì…ë ¥ ì‹œí€€ìŠ¤
- `context_window`: `int` - êµ­ì†Œ ë§¥ë½ ìœˆë„ìš° í¬ê¸° (ê¸°ë³¸ê°’: 5)

#### ì¶œë ¥
- `integrated`: `[B, L, D]` - ë§¥ë½ì´ í†µí•©ëœ í‘œí˜„

#### ë©”íƒ€ë°ì´í„°
```python
{
    'local_context': Tensor,      # [B, L, D]
    'global_context': Tensor,     # [B, L, D]
    'temporal_context': Tensor,   # [B, L, D]
    'hierarchical_context': Tensor,  # [B, L, D]
    'group_context': Tensor,      # [B, L, D]
    'fusion_weights': Tensor      # [B, L, num_contexts]
}
```

### 2.3 íŒŒë¼ë¯¸í„° ì˜ˆì‚°

| ì»´í¬ë„ŒíŠ¸ | íŒŒë¼ë¯¸í„° ìˆ˜ | ë¹„ìœ¨ |
|---------|-----------|------|
| A06 (Sequence Tracker) | ~120K | 18% |
| M01 (Hierarchy Builder) | ~426K | 66% |
| A05 (Grouping Nucleus) | ~100K | 15% |
| **ê¸°ì¡´ ì‹œë“œ í•©ê³„** | **~646K** | **99%** |
| Local Context Encoder | ~3K | 0.5% |
| Global Context Encoder | ~3K | 0.5% |
| Context Fusion (MHA) | ~1K | 0.2% |
| Disambiguator | ~0.5K | 0.1% |
| **ì¶”ê°€ ë ˆì´ì–´ í•©ê³„** | **~7.5K** | **1%** |
| **ì´í•©** | **~653K** | **100%** |

---

## 3. ë‹¨ê³„ë³„ êµ¬í˜„ ê°€ì´ë“œ

### Step 1: í”„ë¡œì íŠ¸ êµ¬ì¡° ì¤€ë¹„

#### 1.1 íŒŒì¼ ìƒì„±

```bash
# ë©”ì¸ êµ¬í˜„ íŒŒì¼
touch seeds/molecular/m06_context_integrator.py

# í™œìš© ì˜ˆì œ íŒŒì¼
mkdir -p examples
touch examples/m06_usage_examples.py

# í…ŒìŠ¤íŠ¸ íŒŒì¼ (ê¸°ì¡´ íŒŒì¼ì— ì¶”ê°€)
# tests/test_molecular_seeds.py
```

#### 1.2 ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
cognitive-seed-framework/
â”œâ”€â”€ seeds/
â”‚   â””â”€â”€ molecular/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ m01_hierarchy_builder.py
â”‚       â”œâ”€â”€ m02_causality_detector.py
â”‚       â”œâ”€â”€ m03_pattern_completer.py
â”‚       â”œâ”€â”€ m04_spatial_transformer.py
â”‚       â””â”€â”€ m06_context_integrator.py  â† ì‹ ê·œ
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ m06_usage_examples.py          â† ì‹ ê·œ
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_molecular_seeds.py        â† ì—…ë°ì´íŠ¸
â””â”€â”€ docs/
    â”œâ”€â”€ M06_RESEARCH_MATERIALS.md      â† ì •ë³´ ìë£Œ
    â””â”€â”€ M06_IMPLEMENTATION_GUIDE.md    â† ë³¸ ë¬¸ì„œ
```

---

### Step 2: Config í´ë˜ìŠ¤ ì‘ì„±

```python
# seeds/molecular/m06_context_integrator.py

from dataclasses import dataclass
from seeds.base import BaseSeed, SeedConfig

@dataclass
class ContextIntegratorConfig(SeedConfig):
    """Context Integrator ì„¤ì •"""
    seed_id: str = "SEED-M06"
    name: str = "Context Integrator"
    level: int = 1
    category: str = "Composition"
    bit_depth: str = "FP8"
    params: int = 650000
    input_dim: int = 128
    output_dim: int = 128
    
    # M06 íŠ¹í™” ì„¤ì •
    num_heads: int = 8
    num_encoder_layers: int = 2
    context_window: int = 5
    dropout: float = 0.1
```

**ì²´í¬í¬ì¸íŠ¸ 1**: âœ… Config í´ë˜ìŠ¤ê°€ SeedConfigë¥¼ ìƒì†í•˜ëŠ”ê°€?

---

### Step 3: ê¸°ë³¸ í´ë˜ìŠ¤ êµ¬ì¡° ì‘ì„±

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional, Tuple

from seeds.atomic import SequenceTracker, GroupingNucleus
from seeds.molecular import HierarchyBuilder

class ContextIntegrator(BaseSeed):
    """
    SEED-M06: Context Integrator
    
    ë‹¤ì¸µì  ë§¥ë½ì„ í†µí•©í•˜ì—¬ ì¤‘ì˜ì„±ì„ í•´ì†Œí•©ë‹ˆë‹¤.
    
    ì£¼ìš” ê¸°ëŠ¥:
    - Multi-scale context encoding (local/global)
    - Hierarchical context integration
    - Multi-head attention fusion
    - Context-based disambiguation
    """
    
    def __init__(
        self,
        input_dim: int = 128,
        num_heads: int = 8,
        num_encoder_layers: int = 2,
        context_window: int = 5,
        dropout: float = 0.1
    ):
        config = ContextIntegratorConfig(
            input_dim=input_dim,
            num_heads=num_heads,
            num_encoder_layers=num_encoder_layers,
            context_window=context_window,
            dropout=dropout
        )
        super().__init__(config)
        
        self.config = config
        
        # Step 4ì—ì„œ êµ¬í˜„í•  ì»´í¬ë„ŒíŠ¸ë“¤
        self._init_atomic_seeds()
        self._init_context_encoders()
        self._init_fusion_module()
        self._init_disambiguator()
```

**ì²´í¬í¬ì¸íŠ¸ 2**: âœ… BaseSeedë¥¼ ìƒì†í•˜ê³  configë¥¼ ì „ë‹¬í•˜ëŠ”ê°€?

---

### Step 4: Atomic Seeds ì´ˆê¸°í™”

```python
def _init_atomic_seeds(self):
    """Atomic/Molecular seeds ì´ˆê¸°í™”"""
    
    # A06: Sequence Tracker (ì‹œê°„ì  ë§¥ë½)
    self.sequence_tracker = SequenceTracker(self.config.input_dim)
    
    # M01: Hierarchy Builder (ê³„ì¸µì  ë§¥ë½)
    self.hierarchy_builder = HierarchyBuilder(self.config.input_dim)
    
    # A05: Grouping Nucleus (ê·¸ë£¹ ë§¥ë½)
    self.grouping_nucleus = GroupingNucleus(self.config.input_dim)
```

**ì²´í¬í¬ì¸íŠ¸ 3**: âœ… ëª¨ë“  êµ¬ì„± ì‹œë“œê°€ ì˜¬ë°”ë¥´ê²Œ importë˜ê³  ì´ˆê¸°í™”ë˜ëŠ”ê°€?

---

### Step 5: Context Encoders êµ¬í˜„

#### 5.1 Local Context Encoder

```python
def _init_context_encoders(self):
    """Local/Global context encoders ì´ˆê¸°í™”"""
    
    # Local context encoder (Transformer)
    local_encoder_layer = nn.TransformerEncoderLayer(
        d_model=self.config.input_dim,
        nhead=self.config.num_heads,
        dim_feedforward=self.config.input_dim * 4,
        dropout=self.config.dropout,
        batch_first=True
    )
    self.local_context_encoder = nn.TransformerEncoder(
        local_encoder_layer,
        num_layers=self.config.num_encoder_layers
    )
    
    # Global context encoder (Transformer)
    global_encoder_layer = nn.TransformerEncoderLayer(
        d_model=self.config.input_dim,
        nhead=self.config.num_heads,
        dim_feedforward=self.config.input_dim * 4,
        dropout=self.config.dropout,
        batch_first=True
    )
    self.global_context_encoder = nn.TransformerEncoder(
        global_encoder_layer,
        num_layers=self.config.num_encoder_layers
    )

def encode_local_context(
    self,
    x: torch.Tensor,
    window_size: Optional[int] = None
) -> torch.Tensor:
    """
    ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ê¸°ë°˜ êµ­ì†Œ ë§¥ë½ ì¸ì½”ë”©
    
    Args:
        x: [B, L, D] - ì…ë ¥ ì‹œí€€ìŠ¤
        window_size: ìœˆë„ìš° í¬ê¸° (Noneì´ë©´ config ê°’ ì‚¬ìš©)
    Returns:
        local_context: [B, L, D] - êµ­ì†Œ ë§¥ë½
    """
    if window_size is None:
        window_size = self.config.context_window
    
    B, L, D = x.shape
    
    # íŒ¨ë”© (ì–‘ìª½ì— window_size // 2ì”©)
    pad_size = window_size // 2
    padded = F.pad(x, (0, 0, pad_size, pad_size))  # [B, L+2*pad_size, D]
    
    # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì¶”ì¶œ
    local_contexts = []
    for i in range(L):
        window = padded[:, i:i+window_size, :]  # [B, window_size, D]
        local_contexts.append(window)
    
    local_contexts = torch.stack(local_contexts, dim=1)  # [B, L, window_size, D]
    
    # Transformer ì¸ì½”ë”© (ê° ìœˆë„ìš° ë…ë¦½ì ìœ¼ë¡œ)
    local_contexts_flat = local_contexts.view(B * L, window_size, D)
    encoded = self.local_context_encoder(local_contexts_flat)  # [B*L, window_size, D]
    
    # í‰ê·  í’€ë§ìœ¼ë¡œ ìœˆë„ìš° ìš”ì•½
    local_context = encoded.mean(dim=1).view(B, L, D)  # [B, L, D]
    
    return local_context

def encode_global_context(self, x: torch.Tensor) -> torch.Tensor:
    """
    ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ ê³ ë ¤í•œ ì „ì—­ ë§¥ë½ ì¸ì½”ë”©
    
    Args:
        x: [B, L, D] - ì…ë ¥ ì‹œí€€ìŠ¤
    Returns:
        global_context: [B, L, D] - ì „ì—­ ë§¥ë½
    """
    # Transformer ì¸ì½”ë”© (ì „ì²´ ì‹œí€€ìŠ¤)
    global_context = self.global_context_encoder(x)  # [B, L, D]
    
    return global_context
```

**ì²´í¬í¬ì¸íŠ¸ 4**: âœ… Local contextê°€ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì˜¬ë°”ë¥´ê²Œ ì¶”ì¶œë˜ëŠ”ê°€?

---

### Step 6: Context Fusion Module êµ¬í˜„

```python
def _init_fusion_module(self):
    """Context fusion module ì´ˆê¸°í™”"""
    
    # Multi-head attention for fusion
    self.context_fusion = nn.MultiheadAttention(
        embed_dim=self.config.input_dim,
        num_heads=self.config.num_heads,
        dropout=self.config.dropout,
        batch_first=True
    )
    
    # Context weighting network (ì„ íƒì )
    self.context_weighter = nn.Sequential(
        nn.Linear(self.config.input_dim * 5, 256),
        nn.ReLU(),
        nn.Dropout(self.config.dropout),
        nn.Linear(256, 5),  # 5ê°œ ë§¥ë½ì— ëŒ€í•œ ê°€ì¤‘ì¹˜
        nn.Softmax(dim=-1)
    )

def fuse_contexts(
    self,
    local: torch.Tensor,
    global_ctx: torch.Tensor,
    temporal: torch.Tensor,
    hierarchical: torch.Tensor,
    group: torch.Tensor
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    ë‹¤ì¤‘ ë§¥ë½ ìœµí•©
    
    Args:
        local: [B, L, D] - êµ­ì†Œ ë§¥ë½
        global_ctx: [B, L, D] - ì „ì—­ ë§¥ë½
        temporal: [B, L, D] - ì‹œê°„ì  ë§¥ë½
        hierarchical: [B, L, D] - ê³„ì¸µì  ë§¥ë½
        group: [B, L, D] - ê·¸ë£¹ ë§¥ë½
    Returns:
        fused: [B, L, D] - ìœµí•©ëœ ë§¥ë½
        weights: [B, L, 5] - ê° ë§¥ë½ì˜ ê°€ì¤‘ì¹˜
    """
    B, L, D = local.shape
    
    # ë°©ë²• 1: Cross-attention ê¸°ë°˜ ìœµí•©
    # localì„ query, ë‚˜ë¨¸ì§€ë¥¼ key/valueë¡œ ì‚¬ìš©
    contexts = torch.stack([global_ctx, temporal, hierarchical, group], dim=2)  # [B, L, 4, D]
    contexts_flat = contexts.reshape(B, L, -1)  # [B, L, 4*D]
    
    # Cross-attention
    fused, attn_weights = self.context_fusion(
        query=local,           # [B, L, D]
        key=contexts_flat,     # [B, L, 4*D]
        value=contexts_flat    # [B, L, 4*D]
    )
    
    # ë°©ë²• 2: Weighted sum (ì¶”ê°€ ì˜µì…˜)
    # ëª¨ë“  ë§¥ë½ ê²°í•©
    all_contexts = torch.cat([local, global_ctx, temporal, hierarchical, group], dim=-1)  # [B, L, 5*D]
    
    # ê°€ì¤‘ì¹˜ ê³„ì‚°
    weights = self.context_weighter(all_contexts)  # [B, L, 5]
    
    # ê°€ì¤‘ í•©ì‚°
    contexts_stacked = torch.stack([local, global_ctx, temporal, hierarchical, group], dim=-1)  # [B, L, D, 5]
    weighted_fused = torch.sum(contexts_stacked * weights.unsqueeze(2), dim=-1)  # [B, L, D]
    
    # ë‘ ë°©ë²• ê²°í•© (ì„ íƒì )
    final_fused = (fused + weighted_fused) / 2
    
    return final_fused, weights
```

**ì²´í¬í¬ì¸íŠ¸ 5**: âœ… ë‹¤ì¤‘ ë§¥ë½ì´ ì˜¬ë°”ë¥´ê²Œ ìœµí•©ë˜ëŠ”ê°€?

---

### Step 7: Disambiguator êµ¬í˜„

```python
def _init_disambiguator(self):
    """Disambiguator ì´ˆê¸°í™”"""
    
    self.disambiguator = nn.Sequential(
        nn.Linear(self.config.input_dim * 3, 256),
        nn.ReLU(),
        nn.Dropout(self.config.dropout),
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Dropout(self.config.dropout),
        nn.Linear(128, self.config.input_dim)
    )

def disambiguate(
    self,
    x: torch.Tensor,
    context: torch.Tensor
) -> torch.Tensor:
    """
    ë§¥ë½ì„ í™œìš©í•œ ì¤‘ì˜ì„± í•´ì†Œ
    
    Args:
        x: [B, L, D] - ì›ë³¸ ì…ë ¥
        context: [B, L, D] - ìœµí•©ëœ ë§¥ë½
    Returns:
        disambiguated: [B, L, D] - ì¤‘ì˜ì„±ì´ í•´ì†Œëœ í‘œí˜„
    """
    # ì›ë³¸, ë§¥ë½, ê·¸ë¦¬ê³  ë‘˜ì˜ ìƒí˜¸ì‘ìš© ê²°í•©
    interaction = x * context  # Element-wise multiplication
    combined = torch.cat([x, context, interaction], dim=-1)  # [B, L, 3*D]
    
    # Disambiguation network
    disambiguated = self.disambiguator(combined)  # [B, L, D]
    
    # Residual connection
    output = x + disambiguated
    
    return output
```

**ì²´í¬í¬ì¸íŠ¸ 6**: âœ… ìƒí˜¸ì‘ìš© íŠ¹ì§•ì´ ì˜¬ë°”ë¥´ê²Œ ê³„ì‚°ë˜ëŠ”ê°€?

---

### Step 8: Forward Pass êµ¬í˜„

```python
def forward(
    self,
    x: torch.Tensor,
    context_window: Optional[int] = None,
    return_metadata: bool = False
) -> torch.Tensor:
    """
    Forward pass
    
    Args:
        x: [B, L, D] - ì…ë ¥ ì‹œí€€ìŠ¤
        context_window: êµ­ì†Œ ë§¥ë½ ìœˆë„ìš° í¬ê¸°
        return_metadata: ë©”íƒ€ë°ì´í„° ë°˜í™˜ ì—¬ë¶€
    Returns:
        integrated: [B, L, D] - ë§¥ë½ì´ í†µí•©ëœ í‘œí˜„
        (ì„ íƒì ) metadata: ì¤‘ê°„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    # 1. Atomic/Molecular seedsë¡œ ë‹¤ì–‘í•œ ë§¥ë½ ì¶”ì¶œ
    temporal_context = self.sequence_tracker(x)        # [B, L, D]
    hierarchical_context = self.hierarchy_builder(x)   # [B, L, D]
    group_context = self.grouping_nucleus(x)           # [B, L, D]
    
    # 2. Local/Global context encoding
    local_context = self.encode_local_context(x, context_window)  # [B, L, D]
    global_context = self.encode_global_context(x)                # [B, L, D]
    
    # 3. Context fusion
    fused_context, fusion_weights = self.fuse_contexts(
        local_context,
        global_context,
        temporal_context,
        hierarchical_context,
        group_context
    )  # [B, L, D], [B, L, 5]
    
    # 4. Disambiguation
    integrated = self.disambiguate(x, fused_context)  # [B, L, D]
    
    if return_metadata:
        metadata = {
            'local_context': local_context,
            'global_context': global_context,
            'temporal_context': temporal_context,
            'hierarchical_context': hierarchical_context,
            'group_context': group_context,
            'fused_context': fused_context,
            'fusion_weights': fusion_weights
        }
        return integrated, metadata
    
    return integrated
```

**ì²´í¬í¬ì¸íŠ¸ 7**: âœ… Forward passê°€ ëª¨ë“  ë‹¨ê³„ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì‹¤í–‰í•˜ëŠ”ê°€?

---

### Step 9: ì¶”ê°€ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ

```python
def get_context_importance(
    self,
    x: torch.Tensor
) -> Dict[str, torch.Tensor]:
    """
    ê° ë§¥ë½ì˜ ì¤‘ìš”ë„ ë¶„ì„
    
    Args:
        x: [B, L, D]
    Returns:
        importance: ë§¥ë½ë³„ ì¤‘ìš”ë„ ë”•ì…”ë„ˆë¦¬
    """
    _, metadata = self.forward(x, return_metadata=True)
    
    fusion_weights = metadata['fusion_weights']  # [B, L, 5]
    
    # í‰ê·  ì¤‘ìš”ë„ ê³„ì‚°
    avg_importance = fusion_weights.mean(dim=[0, 1])  # [5]
    
    importance = {
        'local': avg_importance[0].item(),
        'global': avg_importance[1].item(),
        'temporal': avg_importance[2].item(),
        'hierarchical': avg_importance[3].item(),
        'group': avg_importance[4].item()
    }
    
    return importance

def visualize_context_attention(
    self,
    x: torch.Tensor,
    position: int
) -> Dict[str, torch.Tensor]:
    """
    íŠ¹ì • ìœ„ì¹˜ì˜ ë§¥ë½ attention ì‹œê°í™”
    
    Args:
        x: [B, L, D]
        position: ë¶„ì„í•  ìœ„ì¹˜
    Returns:
        attention_maps: Attention ë§µ ë”•ì…”ë„ˆë¦¬
    """
    _, metadata = self.forward(x, return_metadata=True)
    
    # í•´ë‹¹ ìœ„ì¹˜ì˜ ê° ë§¥ë½ ì¶”ì¶œ
    attention_maps = {
        'local': metadata['local_context'][:, position, :],
        'global': metadata['global_context'][:, position, :],
        'temporal': metadata['temporal_context'][:, position, :],
        'hierarchical': metadata['hierarchical_context'][:, position, :],
        'group': metadata['group_context'][:, position, :]
    }
    
    return attention_maps
```

**ì²´í¬í¬ì¸íŠ¸ 8**: âœ… ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œê°€ ì˜¬ë°”ë¥¸ ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ”ê°€?

---

### Step 10: __init__.py ì—…ë°ì´íŠ¸

```python
# seeds/molecular/__init__.py

from .m01_hierarchy_builder import HierarchyBuilder, create_hierarchy_builder
from .m02_causality_detector import CausalityDetector, create_causality_detector
from .m03_pattern_completer import PatternCompleter
from .m04_spatial_transformer import SpatialTransformer, create_spatial_transformer
from .m06_context_integrator import ContextIntegrator  # ì¶”ê°€

__all__ = [
    "HierarchyBuilder",
    "create_hierarchy_builder",
    "CausalityDetector",
    "create_causality_detector",
    "PatternCompleter",
    "SpatialTransformer",
    "create_spatial_transformer",
    "ContextIntegrator",  # ì¶”ê°€
]
```

**ì²´í¬í¬ì¸íŠ¸ 9**: âœ… Importê°€ ì˜¬ë°”ë¥´ê²Œ ì¶”ê°€ë˜ì—ˆëŠ”ê°€?

---

## 4. í…ŒìŠ¤íŠ¸ ì „ëµ

### 4.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±

```python
# tests/test_molecular_seeds.py

def test_context_integrator_forward(self):
    """M06: Forward pass í…ŒìŠ¤íŠ¸"""
    print("\n[X/Y] Testing Context Integrator - Forward pass...")
    
    seed = ContextIntegrator(input_dim=self.input_dim)
    seed = seed.to(self.device)
    
    # ì…ë ¥: [B, L, D]
    seq_len = 50
    x = torch.randn(self.batch_size, seq_len, self.input_dim).to(self.device)
    
    # Forward
    output = seed(x)
    
    # ì¶œë ¥ shape í™•ì¸
    assert output.shape == (self.batch_size, seq_len, self.input_dim)
    assert not torch.isnan(output).any()
    
    print("âœ“ Forward pass successful")

def test_context_integrator_with_metadata(self):
    """M06: ë©”íƒ€ë°ì´í„° ë°˜í™˜ í…ŒìŠ¤íŠ¸"""
    print("\n[X/Y] Testing Context Integrator - Metadata...")
    
    seed = ContextIntegrator(input_dim=self.input_dim)
    seed = seed.to(self.device)
    
    seq_len = 50
    x = torch.randn(self.batch_size, seq_len, self.input_dim).to(self.device)
    
    # Forward with metadata
    output, metadata = seed(x, return_metadata=True)
    
    # ë©”íƒ€ë°ì´í„° í™•ì¸
    assert 'local_context' in metadata
    assert 'global_context' in metadata
    assert 'temporal_context' in metadata
    assert 'hierarchical_context' in metadata
    assert 'group_context' in metadata
    assert 'fusion_weights' in metadata
    
    # Fusion weights shape í™•ì¸
    assert metadata['fusion_weights'].shape == (self.batch_size, seq_len, 5)
    
    print("âœ“ Metadata return successful")

def test_context_integrator_importance(self):
    """M06: ë§¥ë½ ì¤‘ìš”ë„ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
    print("\n[X/Y] Testing Context Integrator - Context importance...")
    
    seed = ContextIntegrator(input_dim=self.input_dim)
    seed = seed.to(self.device)
    
    seq_len = 50
    x = torch.randn(self.batch_size, seq_len, self.input_dim).to(self.device)
    
    # ì¤‘ìš”ë„ ë¶„ì„
    importance = seed.get_context_importance(x)
    
    # ê²°ê³¼ í™•ì¸
    assert 'local' in importance
    assert 'global' in importance
    assert 'temporal' in importance
    assert 'hierarchical' in importance
    assert 'group' in importance
    
    # ì¤‘ìš”ë„ í•©ì´ 1ì— ê°€ê¹Œìš´ì§€ í™•ì¸
    total = sum(importance.values())
    assert abs(total - 1.0) < 0.01
    
    print("âœ“ Context importance analysis successful")
```

### 4.2 í†µí•© í…ŒìŠ¤íŠ¸

```python
def test_context_integrator_integration(self):
    """M06: ë‹¤ë¥¸ ì‹œë“œì™€ì˜ í†µí•© í…ŒìŠ¤íŠ¸"""
    print("\n[X/Y] Testing Context Integrator - Integration...")
    
    # M06 ìƒì„±
    integrator = ContextIntegrator(input_dim=self.input_dim).to(self.device)
    
    # M03 (Pattern Completer)ì™€ ì—°ê³„
    completer = PatternCompleter(input_dim=self.input_dim).to(self.device)
    
    seq_len = 50
    x = torch.randn(self.batch_size, seq_len, self.input_dim).to(self.device)
    
    # M03 -> M06 íŒŒì´í”„ë¼ì¸
    completed = completer(x)
    integrated = integrator(completed)
    
    # ê²°ê³¼ í™•ì¸
    assert integrated.shape == (self.batch_size, seq_len, self.input_dim)
    assert not torch.isnan(integrated).any()
    
    print("âœ“ Integration with other seeds successful")
```

**ì²´í¬í¬ì¸íŠ¸ 10**: âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ í†µê³¼í•˜ëŠ”ê°€?

---

## 5. ì„±ëŠ¥ ìµœì í™”

### 5.1 ë©”ëª¨ë¦¬ ìµœì í™”

```python
# Gradient checkpointing ì ìš©
from torch.utils.checkpoint import checkpoint

def forward_with_checkpointing(self, x):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ forward pass"""
    
    # Checkpointë¥¼ ì‚¬ìš©í•˜ì—¬ ì¤‘ê°„ í™œì„±í™” ì €ì¥ ìµœì†Œí™”
    temporal_context = checkpoint(self.sequence_tracker, x)
    hierarchical_context = checkpoint(self.hierarchy_builder, x)
    group_context = checkpoint(self.grouping_nucleus, x)
    
    # ... ë‚˜ë¨¸ì§€ ë™ì¼
```

### 5.2 ê³„ì‚° íš¨ìœ¨í™”

```python
# Cached context encoding
@torch.no_grad()
def precompute_global_context(self, x):
    """ì „ì—­ ë§¥ë½ ì‚¬ì „ ê³„ì‚° (ì¶”ë¡  ì‹œ)"""
    return self.encode_global_context(x)

# Batch processing
def forward_batch_efficient(self, x, batch_size=32):
    """ëŒ€ìš©ëŸ‰ ì‹œí€€ìŠ¤ ë°°ì¹˜ ì²˜ë¦¬"""
    B, L, D = x.shape
    
    if L > batch_size:
        # ì‹œí€€ìŠ¤ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        chunks = torch.split(x, batch_size, dim=1)
        results = []
        
        for chunk in chunks:
            result = self.forward(chunk)
            results.append(result)
        
        return torch.cat(results, dim=1)
    else:
        return self.forward(x)
```

### 5.3 ì–‘ìí™” ì¤€ë¹„

```python
# FP8 ì–‘ìí™”ë¥¼ ìœ„í•œ ì¤€ë¹„
def prepare_for_quantization(self):
    """ì–‘ìí™” ì¤€ë¹„"""
    
    # Batch normalization ì¶”ê°€ (ì„ íƒì )
    self.bn_local = nn.BatchNorm1d(self.config.input_dim)
    self.bn_global = nn.BatchNorm1d(self.config.input_dim)
    
    # Quantization-aware training ì„¤ì •
    # torch.quantization.prepare_qat(self, inplace=True)
```

---

## 6. ì°¸ê³  ìë£Œ

### 6.1 ê´€ë ¨ ë¬¸ì„œ

- **ì •ë³´ ìë£Œ**: `docs/M06_RESEARCH_MATERIALS.md`
- **ì„¤ê³„ ê°€ì´ë“œ**: `docs/LEVEL1_IMPLEMENTATION_GUIDE.md`
- **í™œìš© ì˜ˆì œ**: `examples/m06_usage_examples.py`

### 6.2 í•µì‹¬ ë…¼ë¬¸

1. Jin et al., "MoH: Multi-Head Attention as Mixture-of-Head Attention", ICML 2025
2. Yang et al., "Context aware hierarchical attention", Nature 2025
3. Xu et al., "HCF-Net: Hierarchical Context Fusion Network", arXiv 2024

### 6.3 êµ¬í˜„ ì°¸ê³ 

- PyTorch Transformer: https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html
- Multi-head Attention: https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html

---

## ì²´í¬ë¦¬ìŠ¤íŠ¸

êµ¬í˜„ ì™„ë£Œ ì „ í™•ì¸ ì‚¬í•­:

- [ ] Step 1: í”„ë¡œì íŠ¸ êµ¬ì¡° ì¤€ë¹„
- [ ] Step 2: Config í´ë˜ìŠ¤ ì‘ì„±
- [ ] Step 3: ê¸°ë³¸ í´ë˜ìŠ¤ êµ¬ì¡° ì‘ì„±
- [ ] Step 4: Atomic Seeds ì´ˆê¸°í™”
- [ ] Step 5: Context Encoders êµ¬í˜„
- [ ] Step 6: Context Fusion Module êµ¬í˜„
- [ ] Step 7: Disambiguator êµ¬í˜„
- [ ] Step 8: Forward Pass êµ¬í˜„
- [ ] Step 9: ì¶”ê°€ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ
- [ ] Step 10: __init__.py ì—…ë°ì´íŠ¸
- [ ] í…ŒìŠ¤íŠ¸ ì‘ì„± ë° í†µê³¼
- [ ] ë¬¸ì„œ ì‘ì„± (README ì—…ë°ì´íŠ¸)
- [ ] í™œìš© ì˜ˆì œ ì‘ì„±

---

**ì‘ì„±ì¼**: 2025-10-21  
**ì‘ì„±ì**: Manus AI (ëˆ„ìŠ¤ì–‘)  
**ë‹¤ìŒ ì—…ë°ì´íŠ¸**: M06 êµ¬í˜„ ì™„ë£Œ ì‹œ

